#!/usr/bin/env python3
# final_intensive_boost.py

"""
Final intensive boost to achieve professional-grade dataset quality.
Multi-pronged approach to reach 15+ entries/day target.
"""
import sys
import os
import time
import logging
import requests
import hashlib
import feedparser
from datetime import datetime, timedelta
from urllib.parse import quote
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

sys.path.append('src')
from database import CryptoPulseDB

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/final_intensive_boost.log'),
        logging.StreamHandler()
    ]
)

class FinalIntensiveBoost:
    def __init__(self):
        self.db = CryptoPulseDB()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def massive_google_news_sweep(self):
        """Massive Google News collection with comprehensive search terms."""
        logging.info("Starting massive Google News sweep...")
        
        articles = []
        
        # Comprehensive search terms for 2022-2023
        comprehensive_terms = [
            # Price and market terms
            'ethereum price 2022', 'ethereum price 2023', 'ETH cryptocurrency price',
            'ethereum market cap', 'ethereum trading volume', 'ethereum volatility',
            
            # Technology terms
            'ethereum merge 2022', 'ethereum proof of stake', 'ethereum staking rewards',
            'ethereum layer 2', 'ethereum scaling solutions', 'ethereum gas fees',
            
            # DeFi terms
            'ethereum defi protocol', 'ethereum decentralized finance', 'ethereum yield farming',
            'ethereum liquidity pools', 'ethereum smart contracts', 'ethereum dapps',
            
            # Events and news
            'ethereum hack 2022', 'ethereum exploit 2023', 'ethereum security',
            'ethereum regulation SEC', 'ethereum institutional adoption', 'ethereum ETF',
            
            # Ecosystem terms
            'ethereum developers', 'ethereum network upgrade', 'ethereum EIP',
            'ethereum community', 'ethereum foundation', 'vitalik buterin',
            
            # Market sentiment
            'ethereum bear market 2022', 'ethereum bull market 2023', 'ethereum recovery',
            'ethereum crash', 'ethereum rally', 'ethereum investment'
        ]
        
        # Year-specific modifiers
        year_modifiers = ['2022', '2023', '2022 OR 2023']
        
        for base_term in comprehensive_terms[:15]:  # Limit to avoid timeout
            for year_mod in year_modifiers:
                try:
                    search_query = f'{base_term} {year_mod}'
                    encoded_query = quote(search_query)
                    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en&gl=US&ceid=US:en"
                    
                    feed = feedparser.parse(rss_url)
                    
                    for entry in feed.entries[:10]:  # Limit per search
                        try:
                            title = entry.get('title', '')
                            link = entry.get('link', '')
                            summary = entry.get('summary', '')
                            
                            # Parse publication date
                            pub_date = datetime.now()
                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                pub_date = datetime(*entry.published_parsed[:6])
                            
                            # Filter for 2022-2023
                            if not (pub_date.year == 2022 or pub_date.year == 2023):
                                continue
                            
                            # Check for crypto relevance
                            text = (title + " " + summary).lower()
                            if any(keyword in text for keyword in ['ethereum', 'crypto', 'bitcoin', 'blockchain', 'defi']):
                                article_id = hashlib.md5(link.encode()).hexdigest()[:10]
                                month_key = pub_date.strftime('%Y-%m')
                                
                                articles.append({
                                    'id': f"massive_news_{month_key}_{article_id}",
                                    'source': f'massive_news_{month_key}',
                                    'title': title,
                                    'content': summary,
                                    'published_at': pub_date,
                                    'url': link
                                })
                            
                        except Exception as e:
                            logging.debug(f"Failed to parse massive news entry: {e}")
                            continue
                    
                    time.sleep(1)  # Aggressive but respectful rate limiting
                    
                except Exception as e:
                    logging.warning(f"Massive Google News failed for '{search_query}': {e}")
                    continue
        
        return articles
    
    def international_crypto_news(self):
        """Collect international crypto news sources."""
        logging.info("Starting international crypto news collection...")
        
        articles = []
        
        # International crypto news RSS feeds
        international_sources = [
            'https://cointelegraph.com/rss',
            'https://decrypt.co/feed',
            'https://thedefiant.io/feed',
            'https://blockworks.co/feed',
            'https://www.coindesk.com/arc/outboundfeeds/rss/',
        ]
        
        for source_url in international_sources:
            try:
                feed = feedparser.parse(source_url)
                
                for entry in feed.entries[:50]:  # More entries per source
                    try:
                        title = entry.get('title', '')
                        link = entry.get('link', '')
                        summary = entry.get('summary', '')
                        
                        # Parse publication date
                        pub_date = datetime.now()
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_date = datetime(*entry.published_parsed[:6])
                        
                        # Filter for 2022-2023
                        if not (pub_date.year == 2022 or pub_date.year == 2023):
                            continue
                        
                        # Check for Ethereum relevance
                        text = (title + " " + summary).lower()
                        if any(keyword in text for keyword in ['ethereum', 'eth', 'defi', 'smart contract']):
                            article_id = hashlib.md5(link.encode()).hexdigest()[:10]
                            month_key = pub_date.strftime('%Y-%m')
                            
                            articles.append({
                                'id': f"intl_news_{month_key}_{article_id}",
                                'source': f'intl_news_{month_key}',
                                'title': title,
                                'content': summary,
                                'published_at': pub_date,
                                'url': link
                            })
                        
                    except Exception as e:
                        logging.debug(f"Failed to parse international news entry: {e}")
                        continue
                
                time.sleep(3)
                
            except Exception as e:
                logging.warning(f"International news collection failed for {source_url}: {e}")
                continue
        
        return articles
    
    def academic_crypto_papers(self):
        """Collect academic papers and research about Ethereum."""
        logging.info("Starting academic crypto papers collection...")
        
        papers = []
        
        # Academic sources (simplified - would need more sophisticated access)
        academic_searches = [
            'ethereum blockchain research 2022',
            'ethereum smart contracts academic 2023',
            'ethereum consensus mechanism research',
            'ethereum scalability solutions paper',
            'ethereum defi protocol analysis'
        ]
        
        for search_term in academic_searches:
            try:
                # Use Google Scholar-like search
                encoded_query = quote(f'{search_term} filetype:pdf')
                search_url = f"https://www.google.com/search?q={encoded_query}&num=10"
                
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Extract academic paper results
                    for result_div in soup.find_all('div', class_='g')[:5]:
                        try:
                            link_elem = result_div.find('a', href=True)
                            if not link_elem:
                                continue
                                
                            url = link_elem.get('href')
                            
                            # Extract title from search result
                            title_elem = result_div.find('h3')
                            title = title_elem.get_text(strip=True) if title_elem else ""
                            
                            # Extract snippet
                            snippet_elem = result_div.find('span')
                            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                            
                            # Check for Ethereum relevance
                            text = (title + " " + snippet).lower()
                            if any(term in text for term in ['ethereum', 'blockchain', 'smart contract', 'defi']):
                                paper_id = hashlib.md5(url.encode()).hexdigest()[:10]
                                
                                papers.append({
                                    'id': f"academic_{paper_id}",
                                    'source': 'academic_papers_2022_23',
                                    'title': title,
                                    'content': snippet,
                                    'published_at': datetime(2022, 6, 15),  # Approximate
                                    'url': url
                                })
                                
                        except Exception as e:
                            logging.debug(f"Failed to parse academic result: {e}")
                            continue
                
                time.sleep(5)  # Respectful delay for Google
                
            except Exception as e:
                logging.warning(f"Academic search failed for '{search_term}': {e}")
                continue
        
        return papers
    
    def crypto_influencer_content(self):
        """Collect crypto influencer and thought leader content."""
        logging.info("Starting crypto influencer content collection...")
        
        content = []
        
        # Use Google to find influencer content
        influencer_searches = [
            'vitalik buterin ethereum 2022 2023',
            'ethereum foundation blog 2022 2023',
            'ethereum developers medium 2022 2023',
            'consensys ethereum blog 2022 2023',
            'ethereum community updates 2022 2023'
        ]
        
        for search_term in influencer_searches:
            try:
                encoded_query = quote(search_term)
                search_url = f"https://www.google.com/search?q={encoded_query}&num=15"
                
                response = self.session.get(search_url, timeout=15)
                
                if response.status_code == 200:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Extract influencer content results
                    for result_div in soup.find_all('div', class_='g')[:10]:
                        try:
                            link_elem = result_div.find('a', href=True)
                            if not link_elem:
                                continue
                                
                            url = link_elem.get('href')
                            
                            # Extract title from search result
                            title_elem = result_div.find('h3')
                            title = title_elem.get_text(strip=True) if title_elem else ""
                            
                            # Extract snippet
                            snippet_elem = result_div.find('span')
                            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                            
                            # Check for Ethereum relevance
                            text = (title + " " + snippet).lower()
                            if any(term in text for term in ['ethereum', 'eth', 'blockchain', 'crypto']):
                                content_id = hashlib.md5(url.encode()).hexdigest()[:10]
                                
                                content.append({
                                    'id': f"influencer_{content_id}",
                                    'source': 'crypto_influencers_2022_23',
                                    'title': title,
                                    'content': snippet,
                                    'published_at': datetime(2022, 9, 15),  # Approximate
                                    'url': url
                                })
                                
                        except Exception as e:
                            logging.debug(f"Failed to parse influencer result: {e}")
                            continue
                
                time.sleep(5)  # Respectful delay for Google
                
            except Exception as e:
                logging.warning(f"Influencer search failed for '{search_term}': {e}")
                continue
        
        return content
    
    def save_final_data(self, data, source_name):
        """Save final boost data to database."""
        if not data:
            logging.info(f"No data collected from {source_name}")
            return 0
        
        # Use news_articles table for all sources
        df = pd.DataFrame(data)
        df = df.drop_duplicates(subset=['id'])
        
        # Filter for new records
        new_articles = []
        for _, item in df.iterrows():
            if not self.db.record_exists('news_articles', item['id']):
                new_articles.append(item.to_dict())
        
        if new_articles:
            new_df = pd.DataFrame(new_articles)
            inserted = self.db.insert_news_articles(new_df)
            logging.info(f"{source_name}: {len(data)} collected ‚Üí {inserted} new saved")
            return inserted
        
        logging.info(f"{source_name}: {len(data)} collected ‚Üí 0 new (all duplicates)")
        return 0
    
    def final_intensive_campaign(self):
        """Final intensive campaign to reach professional standards."""
        logging.info("=== FINAL INTENSIVE BOOST CAMPAIGN ===")
        
        start_time = datetime.now()
        
        # Run all collection strategies in parallel
        with ThreadPoolExecutor(max_workers=4) as executor:
            # Submit tasks
            tasks = {
                executor.submit(self.massive_google_news_sweep): "massive_google_news",
                executor.submit(self.international_crypto_news): "international_news", 
                executor.submit(self.academic_crypto_papers): "academic_papers",
                executor.submit(self.crypto_influencer_content): "influencer_content"
            }
            
            total_collected = 0
            
            # Collect results
            for future in as_completed(tasks):
                task_name = tasks[future]
                try:
                    result = future.result()
                    saved = self.save_final_data(result, task_name)
                    total_collected += saved
                    logging.info(f"‚úÖ {task_name}: {saved} new entries")
                except Exception as e:
                    logging.error(f"‚ùå {task_name} failed: {e}")
        
        elapsed = datetime.now() - start_time
        
        # Summary
        logging.info("=== FINAL INTENSIVE BOOST COMPLETE ===")
        logging.info(f"Total new entries: {total_collected}")
        logging.info(f"Execution time: {elapsed.total_seconds():.1f} seconds")
        
        return total_collected

def main():
    """Main function for final intensive boost."""
    booster = FinalIntensiveBoost()
    
    print(f"üî• FINAL INTENSIVE BOOST CAMPAIGN")
    print(f"üéØ Goal: Achieve professional-grade 15+ entries/day")
    print(f"üöÄ Strategy: Comprehensive multi-source collection")
    
    total = booster.final_intensive_campaign()
    
    print(f"\nüéâ FINAL BOOST COMPLETE!")
    print(f"üìà Added {total} new entries")
    print(f"üéØ Ready for final dataset quality verification")

if __name__ == "__main__":
    main()