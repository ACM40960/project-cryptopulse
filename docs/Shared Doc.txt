CryptoPulse Project — Shared Coordination Log & Roadmap

🧾 General Info
    • Project Name: CryptoPulse
    • Goal: Predict 1-day Ethereum price movement using regression models on NLP-based metrics from Reddit, Twitter, and crypto news.
    • Primary Device: Linux Mint, 4GB RAM, 256GB ROM
    • Supplementary Resources: Friend’s GPU laptop (for LLM fine-tuning), Google Colab/Kaggle (for lightweight cloud compute)
    • Storage Constraints: Aim to keep raw data under 50GB. Prioritize timestamps.
    • Final Output: Web interface with predictions, insights, and plots.

📁 Folder Structure
CryptoPulse/
├── data/               # Raw & cleaned text data by date
├── db/                 # Unified SQLite DB with timestamped records
├── models/             # Saved model artifacts
├── src/                # Scrapers, metric scripts, utils
├── notebooks/          # Prototyping & EDA
├── scripts/            # Shell or scheduling scripts (manual runs)
├── webapp/             # Flask or Streamlit web interface
├── requirements.txt
└── shared_doc.txt      # Coordination log (this file)

📅 Phase-wise Roadmap
PHASE 1 – Project Bootstrapping
    • Setup folder structure, venv, install dependencies
    • Set timezone consistently (e.g., UTC)
PHASE 2 – Data Collection
    • Reddit Scraper:
        ◦ Subreddits: r/Ethereum, r/CryptoCurrency, r/ethfinance
        ◦ Extract title, body, post time, upvotes, etc.
    • Twitter Scraper (Selenium):
        ◦ Hashtags: #crypto, #nft, #trading, #ethereum
        ◦ Extract: tweet text, likes, RTs, timestamp
    • News Scraper:
        ◦ Sites: coindesk.com, cointelegraph.com, decrypt.co
        ◦ Extract headline, article body, date
    • Store all raw text + metadata in /data/yyyy-mm-dd/
    • Populate unified SQLite table with: id, source, content, engagement, timestamp, raw_json, is_processed
PHASE 3 – Metric Extraction
    • Batch process daily scraped data using score_metrics.py
    • Extract the following per post/article:
        ◦ Sentiment Score (FinBERT or local model)
        ◦ Relevance Score (cosine sim to ETH vector)
        ◦ Volatility Trigger (keyword/LLM flag)
        ◦ Echo Score (cross-platform topic match)
        ◦ Content Depth (links, crypto vocab, length)
    • Store into SQLite DB with computed scores & timestamps
PHASE 4 – Price Labeling
    • Source: CoinGecko/YFinance
    • Align each data day D with %ETH change on D+1
    • Merge metrics and labels into single regression-ready CSV
PHASE 5 – Modeling
    • Start with: Ridge Regression, Random Forest, LightGBM
    • Use time-series cross-validation
    • Include:
        ◦ Past-day rolling window features
        ◦ Time series indicators (ETH volume, volatility, etc.)
PHASE 6 – Web Interface
    • Build lightweight dashboard (Flask or Streamlit)
    • Inputs: date range
    • Outputs:
        ◦ Visuals of metrics
        ◦ Next-day ETH prediction
        ◦ Echo Score + Volatility alerts
PHASE 7 – Advanced Experiments (Optional)
    • Narrative Drift via BERTopic
    • Early Warning Alerts (Bot or visual)
    • Fine-tune LLM on crypto corpus (friend’s GPU)

✅ Design Decisions
    • Store timestamps accurately (created_utc, scraped_at)
    • Keep all sources in one DB but preserve source field
    • Process metrics after scraping (batch jobs)
    • Run manually for now — future scheduling optional
    • Target output = regression model predicting 1-day price change
    • Visual output using Matplotlib

🔗 Must-Have Metrics (Per Text Entry)
    • Sentiment
    • Relevance
    • Volatility Trigger
    • Echo Score
    • Content Depth / Quality

⏲️ Time Handling Protocol
    • All timestamps converted and stored in UTC
    • SQLite indices: (created_utc, source)
    • Folder and DB naming convention includes date
    • Target alignment of ETH price and metrics by date
