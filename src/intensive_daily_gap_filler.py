#!/usr/bin/env python3
# src/intensive_daily_gap_filler.py

"""
Intensive daily gap-filling system to achieve professional-grade dataset density.
Targets 598 low-data days with <5 entries to reach 15+ entries/day average.
"""
import sys
import os
import time
import logging
import sqlite3
import requests
import hashlib
import feedparser
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin
import pandas as pd
from bs4 import BeautifulSoup
from dotenv import load_dotenv

sys.path.append('src')
from database import CryptoPulseDB

load_dotenv()

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/intensive_daily_gap_filler.log'),
        logging.StreamHandler()
    ]
)

class IntensiveDailyGapFiller:
    def __init__(self):
        self.db = CryptoPulseDB()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Get low-data days from database
        self.low_data_days = self.identify_low_data_days()
        
        # Multi-source collection strategies for each day
        self.gap_filling_sources = {
            'google_news_targeted': self.collect_google_news_for_date,
            'cryptocurrency_forums': self.collect_forum_discussions_for_date,
            'reddit_wayback': self.collect_reddit_wayback_for_date,
            'crypto_press_releases': self.collect_press_releases_for_date,
            'github_activity': self.collect_github_activity_for_date,
            'social_media_archives': self.collect_social_archives_for_date
        }
    
    def identify_low_data_days(self):
        """Identify specific days with <5 total entries."""
        logging.info("Identifying low-data days requiring gap filling...")
        
        conn = sqlite3.connect(self.db.db_path)
        
        # Find days with <5 total sentiment entries
        query = """
        WITH daily_data AS (
            SELECT 
                DATE(datetime(created_utc, 'unixepoch')) as date,
                COUNT(*) as reddit_count
            FROM reddit_posts 
            WHERE datetime(created_utc, 'unixepoch') >= '2022-01-01' 
            AND datetime(created_utc, 'unixepoch') < '2024-01-01'
            GROUP BY DATE(datetime(created_utc, 'unixepoch'))
        ),
        daily_news AS (
            SELECT 
                DATE(datetime(published_at, 'unixepoch')) as date,
                COUNT(*) as news_count
            FROM news_articles 
            WHERE datetime(published_at, 'unixepoch') >= '2022-01-01' 
            AND datetime(published_at, 'unixepoch') < '2024-01-01'
            GROUP BY DATE(datetime(published_at, 'unixepoch'))
        )
        SELECT 
            COALESCE(dd.date, dn.date) as date,
            COALESCE(dd.reddit_count, 0) as reddit,
            COALESCE(dn.news_count, 0) as news,
            COALESCE(dd.reddit_count, 0) + COALESCE(dn.news_count, 0) as total
        FROM daily_data dd
        FULL OUTER JOIN daily_news dn ON dd.date = dn.date
        WHERE COALESCE(dd.reddit_count, 0) + COALESCE(dn.news_count, 0) < 5
        ORDER BY date
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        logging.info(f"Found {len(df)} low-data days requiring gap filling")
        return df
    
    def collect_google_news_for_date(self, target_date, entries_needed):
        """Collect Google News articles for specific date."""
        date_str = target_date.strftime('%Y-%m-%d')
        logging.info(f"Collecting Google News for {date_str} (need {entries_needed} entries)")
        
        articles = []
        
        # Date-specific search terms
        search_terms = [
            f'ethereum OR crypto OR blockchain {date_str}',
            f'ETH price OR "ethereum price" {date_str}',
            f'defi OR "decentralized finance" {date_str}',
            f'cryptocurrency market {date_str}',
            f'bitcoin ethereum {date_str}'
        ]
        
        for term in search_terms:
            try:
                # Create date-specific Google News RSS search
                encoded_query = quote(term)
                rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en&gl=US&ceid=US:en"
                
                feed = feedparser.parse(rss_url)
                
                for entry in feed.entries[:5]:  # Limit per search term
                    try:
                        title = entry.get('title', '')
                        link = entry.get('link', '')
                        summary = entry.get('summary', '')
                        
                        # Parse publication date
                        pub_date = datetime.now()
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_date = datetime(*entry.published_parsed[:6])
                        
                        # Check if date matches target (within 1 day tolerance)
                        if abs((pub_date.date() - target_date.date()).days) <= 1:
                            
                            # Check for crypto relevance
                            text = (title + " " + summary).lower()
                            if any(keyword in text for keyword in ['ethereum', 'crypto', 'bitcoin', 'blockchain', 'defi']):
                                article_id = hashlib.md5(link.encode()).hexdigest()[:10]
                                
                                articles.append({
                                    'id': f"gap_fill_{date_str}_{article_id}",
                                    'source': f'gap_fill_news_{date_str}',
                                    'title': title,
                                    'content': summary,
                                    'published_at': pub_date,
                                    'url': link
                                })
                        
                    except Exception as e:
                        logging.debug(f"Failed to parse Google News entry: {e}")
                        continue
                
                time.sleep(2)  # Rate limiting
                
                if len(articles) >= entries_needed:
                    break
                    
            except Exception as e:
                logging.warning(f"Google News search failed for {term}: {e}")
                continue
        
        return articles[:entries_needed]
    
    def collect_forum_discussions_for_date(self, target_date, entries_needed):
        """Collect forum discussions around specific date."""
        date_str = target_date.strftime('%Y-%m-%d')
        logging.info(f"Collecting forum discussions for {date_str}")
        
        posts = []
        
        # Target crypto forums and communities
        forum_sources = [
            'https://www.reddit.com/r/CryptoCurrency/search.json?q=ethereum&sort=new&restrict_sr=1',
            'https://bitcointalk.org/index.php?board=160.0',  # Ethereum board
            'https://ethereum-magicians.org/latest.json',
            'https://research.ethereum.org/latest.json'
        ]
        
        for forum_url in forum_sources[:2]:  # Limit for efficiency
            try:
                if 'reddit.com' in forum_url:
                    # Handle Reddit JSON API
                    response = self.session.get(forum_url, timeout=15)
                    if response.status_code == 200:
                        data = response.json()
                        
                        for post_data in data.get('data', {}).get('children', [])[:5]:
                            post = post_data.get('data', {})
                            created_utc = post.get('created_utc', 0)
                            post_date = datetime.fromtimestamp(created_utc)
                            
                            # Check if date is close to target
                            if abs((post_date.date() - target_date.date()).days) <= 2:
                                title = post.get('title', '')
                                content = post.get('selftext', '')
                                
                                if any(term in (title + content).lower() for term in ['ethereum', 'eth', 'crypto']):
                                    post_id = post.get('id', '')
                                    
                                    posts.append({
                                        'id': f"gap_fill_reddit_{date_str}_{post_id}",
                                        'source': f'gap_fill_reddit_{date_str}',
                                        'title': title,
                                        'content': content[:500],
                                        'published_at': post_date,
                                        'url': f"https://reddit.com{post.get('permalink', '')}"
                                    })
                
                else:
                    # Handle other forum sources
                    response = self.session.get(forum_url, timeout=15)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # Extract forum post titles and links
                        for link in soup.find_all('a', href=True)[:10]:
                            text = link.get_text(strip=True)
                            href = link.get('href')
                            
                            if (len(text) > 10 and href and
                                any(term in text.lower() for term in ['ethereum', 'eth', 'crypto'])):
                                
                                post_id = hashlib.md5(href.encode()).hexdigest()[:10]
                                posts.append({
                                    'id': f"gap_fill_forum_{date_str}_{post_id}",
                                    'source': f'gap_fill_forum_{date_str}',
                                    'title': text,
                                    'content': '',
                                    'published_at': target_date,
                                    'url': urljoin(forum_url, href)
                                })
                
                time.sleep(3)
                
            except Exception as e:
                logging.warning(f"Forum collection failed for {forum_url}: {e}")
                continue
        
        return posts[:entries_needed]
    
    def collect_reddit_wayback_for_date(self, target_date, entries_needed):
        """Use Wayback Machine to find Reddit posts for specific date."""
        date_str = target_date.strftime('%Y%m%d')
        logging.info(f"Collecting Wayback Machine Reddit for {target_date.strftime('%Y-%m-%d')}")
        
        posts = []
        
        # Target Reddit URLs in Wayback Machine
        reddit_urls = [
            f'reddit.com/r/ethereum',
            f'reddit.com/r/cryptocurrency',
            f'reddit.com/r/ethtrader'
        ]
        
        for reddit_url in reddit_urls[:2]:  # Limit for efficiency
            try:
                wayback_url = f"https://web.archive.org/web/{date_str}000000/{reddit_url}"
                response = self.session.get(wayback_url, timeout=20)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Extract Reddit post titles from archived page
                    for link in soup.find_all('a', href=True)[:5]:
                        href = link.get('href')
                        text = link.get_text(strip=True)
                        
                        if (href and '/comments/' in href and len(text) > 15 and
                            any(term in text.lower() for term in ['ethereum', 'eth', 'crypto', 'defi'])):
                            
                            post_id = hashlib.md5(href.encode()).hexdigest()[:10]
                            
                            posts.append({
                                'id': f"gap_fill_wayback_{date_str}_{post_id}",
                                'source': f'gap_fill_wayback_{target_date.strftime("%Y-%m-%d")}',
                                'title': text,
                                'content': '',
                                'published_at': target_date,
                                'url': href
                            })
                
                time.sleep(15)  # Conservative for Wayback Machine
                
            except Exception as e:
                logging.warning(f"Wayback Reddit collection failed for {reddit_url}: {e}")
                continue
        
        return posts[:entries_needed]
    
    def collect_press_releases_for_date(self, target_date, entries_needed):
        """Collect crypto press releases for specific date."""
        date_str = target_date.strftime('%Y-%m-%d')
        
        # Use PR Newswire and other press release sources
        press_sources = [
            'https://www.prnewswire.com/news-releases/cryptocurrency',
            'https://finance.yahoo.com/news/',
            'https://www.businesswire.com/portal/site/home/news/'
        ]
        
        articles = []
        
        # Simple implementation - would need more sophisticated date filtering
        for source in press_sources[:1]:  # Limit for testing
            try:
                response = self.session.get(source, timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    for link in soup.find_all('a', href=True)[:3]:
                        text = link.get_text(strip=True)
                        href = link.get('href')
                        
                        if (len(text) > 20 and href and
                            any(term in text.lower() for term in ['ethereum', 'crypto', 'blockchain'])):
                            
                            article_id = hashlib.md5(href.encode()).hexdigest()[:10]
                            articles.append({
                                'id': f"gap_fill_pr_{date_str}_{article_id}",
                                'source': f'gap_fill_press_{date_str}',
                                'title': text,
                                'content': '',
                                'published_at': target_date,
                                'url': urljoin(source, href)
                            })
                
                time.sleep(5)
                
            except Exception as e:
                logging.debug(f"Press release collection failed: {e}")
                continue
        
        return articles[:entries_needed]
    
    def collect_github_activity_for_date(self, target_date, entries_needed):
        """Collect GitHub activity for specific date."""
        date_str = target_date.strftime('%Y-%m-%d')
        
        # GitHub API search for Ethereum-related activity on specific date
        try:
            search_url = "https://api.github.com/search/issues"
            params = {
                'q': f'ethereum created:{date_str}',
                'sort': 'created',
                'per_page': entries_needed
            }
            
            response = self.session.get(search_url, params=params, timeout=15)
            
            issues = []
            if response.status_code == 200:
                data = response.json()
                
                for issue in data.get('items', []):
                    try:
                        title = issue.get('title', '')
                        body = issue.get('body', '') or ''
                        html_url = issue.get('html_url', '')
                        created_at = issue.get('created_at', '')
                        
                        # Parse date
                        try:
                            created_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                        except:
                            created_date = target_date
                        
                        issue_id = str(issue.get('id', ''))
                        
                        issues.append({
                            'id': f"gap_fill_github_{date_str}_{issue_id}",
                            'source': f'gap_fill_github_{date_str}',
                            'title': title,
                            'content': body[:500],
                            'published_at': created_date.replace(tzinfo=None),
                            'url': html_url
                        })
                        
                    except Exception as e:
                        logging.debug(f"Failed to parse GitHub issue: {e}")
                        continue
            
            return issues[:entries_needed]
            
        except Exception as e:
            logging.warning(f"GitHub collection failed for {date_str}: {e}")
            return []
    
    def collect_social_archives_for_date(self, target_date, entries_needed):
        """Collect social media archives for specific date."""
        # Placeholder for additional social media sources
        # Could include Twitter archives, Telegram channels, Discord archives
        return []
    
    def fill_gaps_for_day(self, target_date, current_entries, target_entries=15):
        """Fill gaps for a specific day using multiple sources."""
        entries_needed = max(0, target_entries - current_entries)
        
        if entries_needed <= 0:
            return 0
        
        logging.info(f"Filling gaps for {target_date.strftime('%Y-%m-%d')}: need {entries_needed} entries")
        
        all_collected = []
        entries_per_source = max(1, entries_needed // len(self.gap_filling_sources))
        
        # Collect from multiple sources
        for source_name, collect_func in self.gap_filling_sources.items():
            try:
                collected = collect_func(target_date, entries_per_source)
                all_collected.extend(collected)
                logging.info(f"{source_name}: collected {len(collected)} entries for {target_date.strftime('%Y-%m-%d')}")
                
                if len(all_collected) >= entries_needed:
                    break
                    
            except Exception as e:
                logging.error(f"Collection failed for {source_name} on {target_date}: {e}")
                continue
        
        # Save collected data
        if all_collected:
            saved = self.save_gap_fill_data(all_collected, target_date.strftime('%Y-%m-%d'))
            logging.info(f"âœ… {target_date.strftime('%Y-%m-%d')}: {saved} new entries saved")
            return saved
        else:
            logging.info(f"âšª {target_date.strftime('%Y-%m-%d')}: No new entries found")
            return 0
    
    def save_gap_fill_data(self, data, date_str):
        """Save gap-fill data to database."""
        if not data:
            return 0
        
        # Use news_articles table for all gap-fill sources
        df = pd.DataFrame(data)
        df = df.drop_duplicates(subset=['id'])
        
        # Filter for new records
        new_articles = []
        for _, item in df.iterrows():
            if not self.db.record_exists('news_articles', item['id']):
                new_articles.append(item.to_dict())
        
        if new_articles:
            new_df = pd.DataFrame(new_articles)
            inserted = self.db.insert_news_articles(new_df)
            return inserted
        
        return 0
    
    def intensive_gap_filling_campaign(self, max_days=50):
        """Main method for intensive gap filling campaign."""
        logging.info("=== INTENSIVE DAILY GAP FILLING CAMPAIGN ===")
        
        start_time = datetime.now()
        total_filled = 0
        days_processed = 0
        
        # Process low-data days, starting with the worst
        low_days_sorted = self.low_data_days.sort_values('total')
        
        for _, row in low_days_sorted.head(max_days).iterrows():
            try:
                target_date = datetime.strptime(row['date'], '%Y-%m-%d')
                current_entries = row['total']
                
                filled = self.fill_gaps_for_day(target_date, current_entries)
                total_filled += filled
                days_processed += 1
                
                # Progress update
                if days_processed % 10 == 0:
                    elapsed = datetime.now() - start_time
                    logging.info(f"Progress: {days_processed} days processed, {total_filled} entries added in {elapsed.total_seconds():.1f}s")
                
            except Exception as e:
                logging.error(f"Failed to fill gaps for {row['date']}: {e}")
                continue
        
        elapsed = datetime.now() - start_time
        
        # Summary
        logging.info("=== GAP FILLING CAMPAIGN COMPLETE ===")
        logging.info(f"Days processed: {days_processed}")
        logging.info(f"Total entries added: {total_filled}")
        logging.info(f"Average entries/day: {total_filled/max(1, days_processed):.1f}")
        logging.info(f"Execution time: {elapsed.total_seconds():.1f} seconds")
        
        return total_filled

def main():
    """Main function for intensive gap filling."""
    filler = IntensiveDailyGapFiller()
    
    print(f"ðŸŽ¯ INTENSIVE DAILY GAP FILLING")
    print(f"ðŸ“Š Target: {len(filler.low_data_days)} low-data days")
    print(f"ðŸŽª Goal: Achieve 15+ entries/day professional standard")
    
    total = filler.intensive_gap_filling_campaign(max_days=20)  # Start with 20 days
    
    print(f"\nâœ… GAP FILLING COMPLETE!")
    print(f"ðŸ“ˆ Added {total} new entries")
    print(f"ðŸŽ¯ Next: Verify improved daily density")

if __name__ == "__main__":
    main()