{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CryptoPulse: A Critical Re-evaluation of Sentiment-Based Financial Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook walks through the entire workflow of the CryptoPulse project. The project's primary goal is not to present a highly accurate prediction model, but to critically re-evaluate the entire process of using social media sentiment for financial prediction, especially when faced with real-world data limitations like data sparsity.\n",
    "\n",
    "We will demonstrate:\n",
    "1.  **The Data Pipeline:** How data is collected and processed.\n",
    "2.  **Feature Engineering:** How sentiment scores are calculated.\n",
    "3.  **The Modelling Process:** Training several models, including a simple, robust baseline and a more complex, but overfit, model.\n",
    "4.  **The Critical Analysis:** How we can identify overfitting and why high accuracy can be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection Pipeline\n",
    "\n",
    "The first step is to collect data from various sources. The following code blocks are based on the scripts found in `src/`. These scripts are designed to be run automatically to continuously collect data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "\n",
    "def get_reddit_data(subreddits, limit=100):\n",
    "    # Add your reddit credentials\n",
    "    reddit = praw.Reddit(client_id='YOUR_CLIENT_ID',\n",
    "                         client_secret='YOUR_CLIENT_SECRET',\n",
    "                         user_agent='YOUR_USER_AGENT')\n",
    "    \n",
    "    posts_data = []\n",
    "    for subreddit_name in subreddits:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        for post in subreddit.hot(limit=limit):\n",
    "            posts_data.append([post.subreddit, post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created])\n",
    "    \n",
    "    return pd.DataFrame(posts_data, columns=['subreddit', 'title', 'score', 'id', 'url', 'num_comments', 'body', 'created'])\n",
    "\n",
    "def get_news_data(urls):\n",
    "    news_data = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            news_data.append([article.title, article.text, article.publish_date])\n",
    "        except Exception as e:\n",
    "            print(f'Error processing article at {url}: {e}')\n",
    "            \n",
    "    return pd.DataFrame(news_data, columns=['title', 'text', 'publish_date'])\n",
    "\n",
    "def get_price_data(ticker, start_date, end_date):\n",
    "    return yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "print('Data collection functions are defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Processing and Feature Engineering\n",
    "\n",
    "Once the data is collected, it needs to be processed and scored. The following code is based on `src/score_metrics.py` and `src/simplified_ml_dataset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def get_sentiment_scores(text_series):\n",
    "    sentiment_pipeline = pipeline('sentiment-analysis')\n",
    "    return text_series.apply(lambda x: sentiment_pipeline(x[:512])[0]['label'] if isinstance(x, str) else 'NEUTRAL')\n",
    "\n",
    "def create_ml_dataset(processed_data, price_data):\n",
    "    # This is a simplified representation of the dataset creation process\n",
    "    # The actual implementation would involve merging, aggregation, and feature creation\n",
    "    print('Creating ML dataset...')\n",
    "    # ... complex data processing logic ...\n",
    "    return pd.DataFrame() # Return a placeholder dataframe\n",
    "\n",
    "print('Data processing and feature engineering functions are defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Training\n",
    "\n",
    "Now we train our models. We will train three types of models:\n",
    "1. A baseline model.\n",
    "2. A simple, robust Logistic Regression model.\n",
    "3. A complex LightGBM model that is prone to overfitting on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "def train_simple_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, X_test, y_test\n",
    "\n",
    "def train_complex_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = lgb.LGBMClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, X_test, y_test\n",
    "\n",
    "print('Model training functions are defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Critical Analysis & Comparison\n",
    "\n",
    "This is the most important part of the analysis. We will compare the models and show how the complex model, despite potentially higher accuracy, is less reliable due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "def plot_feature_importance(model, features):\n",
    "    # Plotting for LightGBM\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_, features)), columns=['Value','Feature'])\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "        plt.title('LightGBM Features (avg over folds)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print('Model analysis and comparison functions are defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the full pipeline of the CryptoPulse project. More importantly, it has shown that a high accuracy score is not the only measure of a model's success. By critically evaluating our models, we have shown that with sparse data, a simpler, more robust model provides a more honest assessment of the predictive power of social media sentiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
